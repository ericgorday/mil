Perception includes point cloud data from stereo and the imaging sonar as well as 2D-RGB data from the forward and down cameras.

The ultimate goal for SubjuGator is to perform all tasks based on abstracted knowledge of what we're trying to do. 

We hope to estimate the location of tasks in advance, and then issue commands like "Go through the start gate" instead of

```
Find red lines
Move left or right until we are pointing between them
Go forward until 5 seconds after cannot see red lines
```

# Reading

* [Awesome Computer Vision](https://github.com/jbhuang0604/awesome-computer-vision#books) - A massive repository of knowledge

# People and Tasks

* David: Visual Odometry using conventional optical flow

* Ralph: Cameras, stereo, vslam

* Gabe: Stereo depth estimation, point-cloud analysis

* Jake & Tess: MonoSLAM

* Matt: Stereo correspondence from underwater caustics

* John Henning: Deep learning for object recognition